{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "from data_utils import tokenize, tokenize_data, load_data, plot, TranscriptionsDataset\n",
    "from model import Encoder, Decoder, Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_transcriptions(model, w_vocab, t_vocab, word):\n",
    "        model.eval()\n",
    "        word_tokenized = tokenize(word)\n",
    "        word_tokenized = [w.upper() for w in word_tokenized]\n",
    "        word_indexed = [w_vocab.token2idx(w) for w in word_tokenized]\n",
    "        word_indexed = torch.LongTensor(word_indexed).to(device)\n",
    "        \n",
    "        outputs = model.predict(word_indexed)\n",
    "        transcriptions = [t_vocab.idx2token(t) for t in outputs]\n",
    "        transcriptions = ''.join(transcriptions)\n",
    "        return transcriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, optimizer, criterion, clip, epoch, train_loss_list):\n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    for batch_idx, (words, sos_transcriptions, eos_transcriptions) in enumerate(train_dataloader):\n",
    "        words = words.to(device)\n",
    "        sos_transcriptions = sos_transcriptions.to(device)\n",
    "        eos_transcriptions = eos_transcriptions.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(words, sos_transcriptions)\n",
    "        output = output.view(-1, output.shape[-1])\n",
    "        eos_transcriptions = eos_transcriptions.view(-1)\n",
    "       \n",
    "        loss = criterion(output.to(device), eos_transcriptions)\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        train_loss_list.append(loss.item())\n",
    "        \n",
    "#         if batch_idx % 50 == 0:\n",
    "#             plot(epoch, batch_idx, train_loss_list)\n",
    "    return epoch_loss / len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_dataloader, criterion):\n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for (words, sos_transcriptions, eos_transcriptions) in test_dataloader:\n",
    "            words = words.to(device)\n",
    "            sos_transcriptions = sos_transcriptions.to(device)\n",
    "            eos_transcriptions = eos_transcriptions.to(device)\n",
    "\n",
    "            output = model(words, sos_transcriptions, 0) # turn off teacher forcing\n",
    "            output = output.view(-1, output.shape[-1])\n",
    "            eos_transcriptions = eos_transcriptions.view(-1)\n",
    "            \n",
    "            output = output.to(device)\n",
    "            loss   = criterion(output, eos_transcriptions)\n",
    "            \n",
    "            epoch_loss += loss.item()                                   \n",
    "    return epoch_loss / len(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, test_dataloader = load_data()\n",
    "w_vocab = pickle.load(open('word_vocab.pickle', 'rb'))\n",
    "t_vocab = pickle.load(open('transcription_vocab.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(56, 100)\n",
       "    (LSTM): LSTM(100, 64, batch_first=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(56, 100)\n",
       "    (LSTM): LSTM(100, 64, batch_first=True)\n",
       "    (out): Linear(in_features=64, out_features=56, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_DIM   = len(w_vocab)\n",
    "OUTPUT_DIM  = len(t_vocab)\n",
    "ENC_EMB_DIM = 100\n",
    "DEC_EMB_DIM = 100\n",
    "HID_DIM  = 64\n",
    "N_LAYERS = 1\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS).to(device)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS).to(device)\n",
    "model = Seq2Seq(enc, dec).to(device)\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX   = t_vocab.token2idx('<pad>')\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# N_EPOCHS = 50\n",
    "# CLIP = 1\n",
    "# data = tokenize_data()\n",
    "# best_test_loss  = float('inf')\n",
    "# train_loss_list = []\n",
    "# for epoch in range(N_EPOCHS):\n",
    "    \n",
    "#     start_time = time.time()\n",
    "#     train_loss = train(model, train_dataloader, optimizer, criterion, CLIP, epoch, train_loss_list)\n",
    "#     test_loss  = evaluate(model, test_dataloader, criterion)\n",
    "#     end_time   = time.time()\n",
    "    \n",
    "#     epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "#     if test_loss < best_test_loss:\n",
    "#         best_test_loss = test_loss\n",
    "#         torch.save(model.state_dict(), 'checkpoint.pth')\n",
    "        \n",
    "#     print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "#     print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "#     print(f'\\t Val. Loss: {test_loss:.3f} |  Val. PPL: {math.exp(test_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD:          CONTRARIANS\n",
      "GENERATED:     KAANTRAHHHAYNERZ\n",
      "TRANSCRIPTION: CONTRARIANS\n",
      "\n",
      "\n",
      "WORD:          CYLINDRICAL\n",
      "GENERATED:     KIHLIHNDRIHKAHL\n",
      "TRANSCRIPTION: CYLINDRICAL\n",
      "\n",
      "\n",
      "WORD:          PRESSWOODS<pad>\n",
      "GENERATED:     PREHSWEYDOWSIHN\n",
      "TRANSCRIPTION: PRESSWOODS<pad>\n",
      "\n",
      "\n",
      "WORD:          SOFTSPOKEN<pad>\n",
      "GENERATED:     SAAPSKAHBAHLSIHNG\n",
      "TRANSCRIPTION: SOFTSPOKEN<pad>\n",
      "\n",
      "\n",
      "WORD:          WITTENMYER<pad>\n",
      "GENERATED:     WIHTAHNMAYGREYT\n",
      "TRANSCRIPTION: WITTENMYER<pad>\n",
      "\n",
      "\n",
      "WORD:          INFATUATES<pad>\n",
      "GENERATED:     IHNFAHGTEYSHAHTIHNG\n",
      "TRANSCRIPTION: INFATUATES<pad>\n",
      "\n",
      "\n",
      "WORD:          SHOREWARD<pad><pad>\n",
      "GENERATED:     SHAORDAHMAARGAHND\n",
      "TRANSCRIPTION: SHOREWARD<pad><pad>\n",
      "\n",
      "\n",
      "WORD:          PARCPLACE<pad><pad>\n",
      "GENERATED:     PAARPKAHLAHSEYBAHND\n",
      "TRANSCRIPTION: PARCPLACE<pad><pad>\n",
      "\n",
      "\n",
      "WORD:          SHAREWARE<pad><pad>\n",
      "GENERATED:     SHERAHDRAHFAYAHLZAHZ\n",
      "TRANSCRIPTION: SHAREWARE<pad><pad>\n",
      "\n",
      "\n",
      "WORD:          DERIDDER<pad><pad><pad>\n",
      "GENERATED:     DERIHDERBAEKTAHVAHND\n",
      "TRANSCRIPTION: DERIDDER<pad><pad><pad>\n",
      "\n",
      "\n",
      "WORD:          MACUMOLO<pad><pad><pad>\n",
      "GENERATED:     MAHKYUWMAHGLAEMAHJHE\n",
      "TRANSCRIPTION: MACUMOLO<pad><pad><pad>\n",
      "\n",
      "\n",
      "WORD:          MILLIONS<pad><pad><pad>\n",
      "GENERATED:     MIHLIYAHNGMAHBIHNIHN\n",
      "TRANSCRIPTION: MILLIONS<pad><pad><pad>\n",
      "\n",
      "\n",
      "WORD:          GATEWAYS<pad><pad><pad>\n",
      "GENERATED:     GAETAHMERJHAHDAHSTIH\n",
      "TRANSCRIPTION: GATEWAYS<pad><pad><pad>\n",
      "\n",
      "\n",
      "WORD:          CHOPPER<pad><pad><pad><pad>\n",
      "GENERATED:     CHAAPERPAEDAHBAHLIHN\n",
      "TRANSCRIPTION: CHOPPER<pad><pad><pad><pad>\n",
      "\n",
      "\n",
      "WORD:          YEAGLEY<pad><pad><pad><pad>\n",
      "GENERATED:     YIYGAHLIHGMEYTEYVAHD\n",
      "TRANSCRIPTION: YEAGLEY<pad><pad><pad><pad>\n",
      "\n",
      "\n",
      "WORD:          CARBONI<pad><pad><pad><pad>\n",
      "GENERATED:     KAARBAHGLEYDEYTIHNGI\n",
      "TRANSCRIPTION: CARBONI<pad><pad><pad><pad>\n",
      "\n",
      "\n",
      "WORD:          FIDDLER<pad><pad><pad><pad>\n",
      "GENERATED:     FIHDAHLBERKAHBAHNAHD\n",
      "TRANSCRIPTION: FIDDLER<pad><pad><pad><pad>\n",
      "\n",
      "\n",
      "WORD:          TRIPP'S<pad><pad><pad><pad>\n",
      "GENERATED:     TRIHPMAESGAHNGREYTIH\n",
      "TRANSCRIPTION: TRIPP'S<pad><pad><pad><pad>\n",
      "\n",
      "\n",
      "WORD:          BLAMES<pad><pad><pad><pad><pad>\n",
      "GENERATED:     BLEYMPAHKEYDAHBAELAH\n",
      "TRANSCRIPTION: BLAMES<pad><pad><pad><pad><pad>\n",
      "\n",
      "\n",
      "WORD:          PICHON<pad><pad><pad><pad><pad>\n",
      "GENERATED:     PIHKAHNSTAARPAHDAHVA\n",
      "TRANSCRIPTION: PICHON<pad><pad><pad><pad><pad>\n",
      "\n",
      "\n",
      "WORD:          KURTIS<pad><pad><pad><pad><pad>\n",
      "GENERATED:     KAHTRIHKBAEKAHBAHLAH\n",
      "TRANSCRIPTION: KURTIS<pad><pad><pad><pad><pad>\n",
      "\n",
      "\n",
      "WORD:          ORINDA<pad><pad><pad><pad><pad>\n",
      "GENERATED:     AORIYNDAHGAEMAHDEYTI\n",
      "TRANSCRIPTION: ORINDA<pad><pad><pad><pad><pad>\n",
      "\n",
      "\n",
      "WORD:          KUHNLE<pad><pad><pad><pad><pad>\n",
      "GENERATED:     KAHLKAHNBAELDAHGEYTI\n",
      "TRANSCRIPTION: KUHNLE<pad><pad><pad><pad><pad>\n",
      "\n",
      "\n",
      "WORD:          WEHMAN<pad><pad><pad><pad><pad>\n",
      "GENERATED:     WEHMAHNGKAABRAHMAHDI\n",
      "TRANSCRIPTION: WEHMAN<pad><pad><pad><pad><pad>\n",
      "\n",
      "\n",
      "WORD:          BOYLE<pad><pad><pad><pad><pad><pad>\n",
      "GENERATED:     BOYLAHKBAELDERVAHJHE\n",
      "TRANSCRIPTION: BOYLE<pad><pad><pad><pad><pad><pad>\n",
      "\n",
      "\n",
      "WORD:          CARMA<pad><pad><pad><pad><pad><pad>\n",
      "GENERATED:     KAAMRAHMBAEKERVEYTIH\n",
      "TRANSCRIPTION: CARMA<pad><pad><pad><pad><pad><pad>\n",
      "\n",
      "\n",
      "WORD:          FLACK<pad><pad><pad><pad><pad><pad>\n",
      "GENERATED:     FLAEKKAEPAELDAHDEYVA\n",
      "TRANSCRIPTION: FLACK<pad><pad><pad><pad><pad><pad>\n",
      "\n",
      "\n",
      "WORD:          LODES<pad><pad><pad><pad><pad><pad>\n",
      "GENERATED:     LOWDAHSAEMBEREYDAHBA\n",
      "TRANSCRIPTION: LODES<pad><pad><pad><pad><pad><pad>\n",
      "\n",
      "\n",
      "WORD:          BRUNO<pad><pad><pad><pad><pad><pad>\n",
      "GENERATED:     BRUWNAEPKAHVERDEYTAH\n",
      "TRANSCRIPTION: BRUNO<pad><pad><pad><pad><pad><pad>\n",
      "\n",
      "\n",
      "WORD:          APRIL<pad><pad><pad><pad><pad><pad>\n",
      "GENERATED:     AAPRIHPLAEDEREYTIHVA\n",
      "TRANSCRIPTION: APRIL<pad><pad><pad><pad><pad><pad>\n",
      "\n",
      "\n",
      "WORD:          FARE<pad><pad><pad><pad><pad><pad><pad>\n",
      "GENERATED:     FEHRAHPAABKAALEYDAHJ\n",
      "TRANSCRIPTION: FARE<pad><pad><pad><pad><pad><pad><pad>\n",
      "\n",
      "\n",
      "WORD:          EV<pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "GENERATED:     EHVTAHPAEGREYDAHBLAE\n",
      "TRANSCRIPTION: EV<pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('checkpoint.pth'))\n",
    "for (words, y_in, y_out) in test_dataloader:\n",
    "    for i in words:\n",
    "        output = generate_transcriptions(model, w_vocab, t_vocab, ''.join(w_vocab.idx2sent(i.tolist())))\n",
    "        print('WORD:         ', ''.join(w_vocab.idx2sent(i.tolist())))\n",
    "        print('GENERATED:    ', output)    \n",
    "        print('TRANSCRIPTION:', ''.join(t_vocab.idx2sent(i.tolist())))\n",
    "        print('\\n')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-345a4fc3d4b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(len(data)):\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in test_dataloader:\n",
    "    print(w_vocab.idx2sent(i[0][0].tolist()))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i[0][0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
