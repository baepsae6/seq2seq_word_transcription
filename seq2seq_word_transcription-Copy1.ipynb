{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from vocab import Vocab\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import nltk\n",
    "device = torch.device('cuda')\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "from data_utils import tokenize\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(epoch, step, train_losses):\n",
    "    clear_output()\n",
    "    plt.title(f'Epochs {epoch}, step {step}')\n",
    "    plt.plot(train_losses)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data  = pd.read_csv('data/transcriptions/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Word</th>\n",
       "      <th>Transcription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>KNOXVILLE</td>\n",
       "      <td>N AA K S V IH L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>MOVIEGOING</td>\n",
       "      <td>M UW V IY G OW IH NG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>PHOTOSYNTHESIS</td>\n",
       "      <td>F OW T OW S IH N TH AH S IH S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>DELIO</td>\n",
       "      <td>D EY L IY OW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>SWIVELED</td>\n",
       "      <td>S W IH V AH L D</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id            Word                  Transcription\n",
       "0   1       KNOXVILLE                N AA K S V IH L\n",
       "1   2      MOVIEGOING           M UW V IY G OW IH NG\n",
       "2   3  PHOTOSYNTHESIS  F OW T OW S IH N TH AH S IH S\n",
       "3   4           DELIO                   D EY L IY OW\n",
       "4   5        SWIVELED                S W IH V AH L D"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(word):\n",
    "    return [char for char in word if char!=' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Word'] = data['Word'].apply(lambda row: tokenize(row) if type(row)!= float else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[(data['Word']!=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Transcription'] = data['Transcription'].apply(lambda row: tokenize(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, i in enumerate(data['Word']):\n",
    "    for j in i: \n",
    "        if type(j) != str:\n",
    "            print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranscriptionsDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.t_vocab = pickle.load(open('transcription_vocab.pickle', 'rb'))\n",
    "        self.w_vocab = pickle.load(open('word_vocab.pickle', 'rb'))\n",
    "        self.word = self.data['Word'].values\n",
    "        self.transcription = self.data['Transcription'].values\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        word = self.word[idx]\n",
    "        transcription = self.transcription[idx]\n",
    "        word  = self.w_vocab.sent2idx(word)\n",
    "        transcription = self.t_vocab.sent2idx(transcription)\n",
    "        sample = {'word': word, 'transcription': transcription}\n",
    "        return sample\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def collate_fn(self, dicts): \n",
    "        pad_token = self.t_vocab.token2idx('<pad>')\n",
    "        sos_token = self.t_vocab.token2idx('<sos>')\n",
    "        eos_token = self.t_vocab.token2idx('<eos>')\n",
    "        words_padded = []\n",
    "        corpus_size = len(dicts)\n",
    "        len_words_list = [len(d['word']) for d in dicts]\n",
    "        words_list = [d['word'] for d in dicts]\n",
    "        \n",
    "        len_transcriptions_list = [len(d['transcription']) for d in dicts]\n",
    "        transcriptions = [i['transcription'] for i in dicts]\n",
    "       \n",
    "        sorted_len_words, sorted_words, sorted_len_transcriptions, sorted_transcriptions = list(zip(*sorted(zip(\n",
    "            len_words_list, words_list, len_transcriptions_list, transcriptions),key=lambda x: x[0] ,reverse=True)))      \n",
    "        max_lens_word = max(sorted_len_words)\n",
    "        max_lens_transcription = max(sorted_len_transcriptions)\n",
    "\n",
    "        \n",
    "        words_padded = [sorted_words[i] + [pad_token] * (max_lens_word - sorted_len_words[i]) for i in range(corpus_size)]\n",
    "        sos_transcriptions_padded = [[sos_token] + sorted_transcriptions[i] + [pad_token] * (max_lens_transcription - sorted_len_transcriptions[i]) for i in range(corpus_size)]\n",
    "        eos_transcriptions_padded = [sorted_transcriptions[i] + [eos_token] + [pad_token] * (max_lens_transcription - sorted_len_transcriptions[i]) for i in range(corpus_size)]\n",
    "        words_padded = torch.LongTensor(words_padded)\n",
    "        \n",
    "        sos_transcriptions_padded = torch.LongTensor(sos_transcriptions_padded)\n",
    "        eos_transcriptions_padded = torch.LongTensor(eos_transcriptions_padded)\n",
    "        \n",
    "        return (words_padded, sos_transcriptions_padded, eos_transcriptions_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(data, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TranscriptionsDataset(X_train)\n",
    "test_dataset = TranscriptionsDataset(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_vocab = pickle.load(open('word_vocab.pickle', 'rb'))\n",
    "t_vocab = pickle.load(open('transcription_vocab.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=32,\n",
    "                            shuffle=True, collate_fn=train_dataset.collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32,\n",
    "                            shuffle=False, collate_fn=test_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for (words, t_e, t_s) in train_dataloader:\n",
    "#     print(words, t_e, t_s)\n",
    "#     break\n",
    "# print(words.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,input_dim, enc_embed_dim, hidden_dim, n_layers):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.enc_embed_dim = enc_embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(input_dim, enc_embed_dim)\n",
    "        self.GRU = nn.GRU(enc_embed_dim, hidden_dim, n_layers, batch_first=True)\n",
    "    \n",
    "    def forward(self,input_seq):\n",
    "        embedded = self.embedding(input_seq) # input_seq = (5, seq_len) embedded = (5, seq_len, 256)\n",
    "        outputs, hidden = self.GRU(embedded) # outputs = (5, seq_len, 256)\n",
    "        return outputs, hidden\n",
    "    \n",
    "class AttentionDecoder(nn.Module):\n",
    "    def __init__(self, output_dim, dec_embed_dim, hidden_dim, n_layers):\n",
    "        super(AttentionDecoder, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.dec_embed_dim = dec_embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        #layers\n",
    "        self.embedding = nn.Embedding(output_dim, dec_embed_dim)\n",
    "        self.GRU = nn.GRU(dec_embed_dim, hidden_dim, n_layers, batch_first=True)\n",
    "        self.attention_combined = nn.Linear(hidden_dim * 2 + dec_embed_dim, hidden_dim)\n",
    "        self.out = nn.Linear(hidden_dim*2, output_dim)\n",
    "        \n",
    "    def forward(self, input_step, encoder_outputs, hidden):\n",
    "        #input_step = input_step.unsqueeze(1)\n",
    "        # hidden = (batch_size, max_len1, hidden_size)\n",
    "        # encoder_outputs = (batch_size, max_len2, hidden_size)\n",
    "        # a = hidden x encoder_outputs\n",
    "        embedded = self.embedding(input_step)  # embedded = (32, seq_len, 256)\n",
    "        output, hidden = self.GRU(embedded, hidden)\n",
    "        #hidden = hidden.transpose(0,1)\n",
    "        a = torch.bmm(output, encoder_outputs.transpose(1,2)) # a = (batch_size, max_len1, max_len2)\n",
    "        \n",
    "        attn_weights = F.softmax(a, dim=0) \n",
    "        # context should be = (batch_size, max_len2, hidden_size)\n",
    "        # context = encoder_outputs x attn_weights\n",
    "        \n",
    "        context = torch.bmm(attn_weights, encoder_outputs) # (32, 5(max_len from hidden), 64)\n",
    "        #attention_combined = self.attention_combined(torch.cat((context, embedded), 2))\n",
    "        #print(attention_combined.shape)\n",
    "        output = torch.cat((context, output), 2)\n",
    "        prediction = self.out(output)\n",
    "        return prediction, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(w_vocab)\n",
    "OUTPUT_DIM = len(t_vocab)\n",
    "ENC_EMB_DIM = 100\n",
    "DEC_EMB_DIM = 100\n",
    "HID_DIM = 64\n",
    "N_LAYERS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS).to(device)\n",
    "dec = AttentionDecoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 5, 13])\n",
      "torch.Size([32, 64, 13])\n"
     ]
    }
   ],
   "source": [
    "# output = (32, 5, 8)\n",
    "e = torch.randn(32, 13, 64)\n",
    "h = torch.randn(32, 5, 64)\n",
    "e = e.transpose(1,2)\n",
    "res = torch.bmm(h, e)\n",
    "print(res.shape)\n",
    "print(e.shape)\n",
    "res2 = torch.bmm(res, e.transpose(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, word, transcription, teacher_forcing_ratio = 0.5):     \n",
    "        # transcription = (32, max_len)\n",
    "        batch_size = transcription.shape[0]\n",
    "        max_len = transcription.shape[1]\n",
    "        transcription_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        # tensor to store decoder outputs\n",
    "        outputs = torch.zeros(max_len, batch_size, transcription_vocab_size)\n",
    "        \n",
    "        # last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "        encoder_outputs, hidden = self.encoder(word) # hidden = (1, seq_len, 256)\n",
    "        # first input to the decoder is the <sos> tokens\n",
    "    \n",
    "        input = transcription\n",
    "        \n",
    "        # input.size() = (32)\n",
    "        output, hidden = self.decoder(input, encoder_outputs, hidden)\n",
    "#         for t in range(1, max_len):\n",
    "            \n",
    "#             output, hidden = self.decoder(input, hidden)         \n",
    "#             outputs[t] = output\n",
    "#             teacher_force = random.random() < teacher_forcing_ratio\n",
    "#             top1 = output.max(1)[1]\n",
    "#             input = (transcription[:,t] if teacher_force else top1)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def predict(self, word):\n",
    "        \n",
    "        word = word.unsqueeze(0) # word = (1, 4) where 4 is len of word\n",
    "        \n",
    "        outputs, hidden = self.encoder(word)\n",
    "    \n",
    "        out_input = torch.LongTensor([2]).to(device) # input = (1,) where 1 is <sos> token\n",
    "        out_input = out_input.unsqueeze(0)\n",
    "        \n",
    "        preds = torch.LongTensor([[2]])\n",
    "        for _ in range(20):\n",
    "            output, hidden = self.decoder(preds, outputs, hidden) # output, hidden = (1, 1, 64)\n",
    "            #print(output) # \n",
    "            print(output.shape)\n",
    "            #print(torch.argmax(output)) # 10\n",
    "            output = torch.argmax(output, -1).unsqueeze(0)\n",
    "            # our_value = output.item()\n",
    "            print(output.shape)\n",
    "            last_token = output[0][0][-1].item()\n",
    "            if last_token == 3:\n",
    "                break\n",
    "            # preds.append(our_value)\n",
    "            \n",
    "            out_input = torch.LongTensor([last_token]).unsqueeze(0).to(device)\n",
    "            #print('output', out_input.shape)\n",
    "            #print('preds', preds.shape)\n",
    "            preds = torch.cat((preds, out_input), 1)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2Seq(enc, dec).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#outputs = model.predict(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_transcriptions(model, w_vocab, t_vocab, word):\n",
    "        model.eval()\n",
    "        \n",
    "        word_tokenized = tokenize(word)\n",
    "        word_tokenized = [w.upper() for w in word_tokenized]\n",
    "        word_indexed = [w_vocab.token2idx(w) for w in word_tokenized]\n",
    "        word_indexed = torch.LongTensor(word_indexed).to(device)\n",
    "        \n",
    "        outputs = model.predict(word_indexed)\n",
    "        transcriptions = [t_vocab.idx2token(t).lower() for t in outputs]\n",
    "        transcriptions = ' '.join(transcriptions)\n",
    "        return transcriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(56, 100)\n",
       "    (GRU): GRU(100, 64, batch_first=True)\n",
       "  )\n",
       "  (decoder): AttentionDecoder(\n",
       "    (embedding): Embedding(56, 100)\n",
       "    (GRU): GRU(100, 64, batch_first=True)\n",
       "    (attention_combined): Linear(in_features=228, out_features=64, bias=True)\n",
       "    (out): Linear(in_features=128, out_features=56, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX = t_vocab.token2idx('<pad>')\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, optimizer, criterion, clip, epoch, train_loss_list):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for batch_idx, (words, sos_transcriptions, eos_transcriptions) in enumerate(train_dataloader):\n",
    "        \n",
    "        words = words.to(device)\n",
    "        sos_transcriptions = sos_transcriptions.to(device)\n",
    "        eos_transcriptions = eos_transcriptions.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(words, sos_transcriptions)\n",
    "              \n",
    "        #trg = [batch size, trg sent len]\n",
    "        #output = [trg sent len, batch size, output dim]\n",
    "        \n",
    "        output = output.view(-1, output.shape[-1])\n",
    "        eos_transcriptions = eos_transcriptions.view(-1)\n",
    "        \n",
    "        #trg = [(trg sent len - 1) * batch size]\n",
    "        #output = [(trg sent len - 1) * batch size, output dim]\n",
    "        loss = criterion(output.to(device), eos_transcriptions)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        train_loss_list.append(loss.item())\n",
    "        \n",
    "        if batch_idx % 50 == 0:\n",
    "            plot(epoch, batch_idx, train_loss_list)\n",
    "            \n",
    "    return epoch_loss / len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_dataloader, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for (words, sos_transcriptions, eos_transcriptions) in test_dataloader:\n",
    "            words = words.to(device)\n",
    "            sos_transcriptions = sos_transcriptions.to(device)\n",
    "            eos_transcriptions = eos_transcriptions.to(device)\n",
    "\n",
    "            output = model(words, sos_transcriptions, 0) #turn off teacher forcing\n",
    "\n",
    "            # trg = [batch size, trg sent len,]\n",
    "            # output = [trg sent len, batch size, output dim]\n",
    "            \n",
    "            output = output.view(-1, output.shape[-1])\n",
    "            eos_transcriptions = eos_transcriptions.view(-1)\n",
    "            \n",
    "            # trg = [(trg sent len - 1) * batch size]\n",
    "            # output = [(trg sent len - 1) * batch size, output dim]\n",
    "            output = output.to(device)\n",
    "            loss = criterion(output, eos_transcriptions)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "                                            \n",
    "    return epoch_loss / len(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of backend CUDA but got backend CPU for argument #3 'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-2f8b8796717e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'checkpoint_dec.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mgenerations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_transcriptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'apple'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# print(generations)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-48d76cb84b43>\u001b[0m in \u001b[0;36mgenerate_transcriptions\u001b[0;34m(model, w_vocab, t_vocab, word)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mword_indexed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_indexed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_indexed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mtranscriptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx2token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mtranscriptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranscriptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-d1d764ec33dd>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# output, hidden = (1, 1, 64)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0;31m#print(output) #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-dbf7693c5062>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_step, encoder_outputs, hidden)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# encoder_outputs = (batch_size, max_len2, hidden_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# a = hidden x encoder_outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0membedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_step\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# embedded = (32, seq_len, 256)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGRU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m#hidden = hidden.transpose(0,1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m         return F.embedding(\n\u001b[1;32m    116\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1504\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1505\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1506\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of backend CUDA but got backend CPU for argument #3 'index'"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 20\n",
    "CLIP = 1\n",
    "\n",
    "best_test_loss = float('inf')\n",
    "train_loss_list = []\n",
    "for epoch in range(1, N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_dataloader, optimizer, criterion, CLIP, epoch, train_loss_list)\n",
    "    \n",
    "    test_loss = evaluate(model, test_dataloader, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if test_loss < best_test_loss:\n",
    "        best_test_loss = test_loss\n",
    "        torch.save(model.state_dict(), 'checkpoint.pth')\n",
    "        torch.save(model.encoder.state_dict(), 'checkpoint_enc.pth')\n",
    "        torch.save(model.decoder.state_dict(), 'checkpoint_dec.pth')\n",
    "    \n",
    "    generations = generate_transcriptions(model, w_vocab, t_vocab, 'apple')\n",
    "    \n",
    "    # print(generations)\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {test_loss:.3f} |  Val. PPL: {math.exp(test_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS)\n",
    "dec = AttentionDecoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(object):\n",
    "    def __init__(self):\n",
    "        self.encoder = enc\n",
    "        self.encoder.load_state_dict(torch.load('checkpoint_enc.pth'))\n",
    "        self.decoder = dec\n",
    "        self.decoder.load_state_dict(torch.load('checkpoint_dec.pth'))\n",
    "        self.model = model\n",
    "        self.model.load_state_dict(torch.load('checkpoint.pth'))\n",
    "        self.w_vocab = w_vocab\n",
    "        self.t_vocab = t_vocab\n",
    "        \n",
    "    def generate_transcriptions(self, word):\n",
    "        \n",
    "        self.model.eval()\n",
    "        word_tokenized = tokenize(word)\n",
    "        word_tokenized = [w.upper() for w in word_tokenized]\n",
    "        word_indexed = [w_vocab.token2idx(w) for w in word_tokenized]\n",
    "        word_indexed = torch.LongTensor(word_indexed).to(device)\n",
    "        \n",
    "        outputs = model.predict(word_indexed)\n",
    "        transcriptions = [t_vocab.idx2token(t) for t in outputs]\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = g.generate_transcriptions('bottle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = []\n",
    "for i in t:\n",
    "    j = t_vocab.idx2token(i)\n",
    "    tr.append(j.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
